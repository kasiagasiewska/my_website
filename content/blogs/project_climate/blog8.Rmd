---
title: "Session 4: Homework 2"
author: "Aman Sharma, Christoph Sieker, Kasia Gasiewska, Peter Moravec, Philippe Schrage, Satyam Gorry"
date: "04/10/2020"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
```

# Climate change and temperature anomalies 

```{r weather_data, cache=TRUE}
# Loading the file

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v3/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

```{r tidyweather}
# Selecting the relevant variables

weather_select <- weather %>%
  select(-c(14, 15, 16, 17, 18, 19))

# Adjusting the selected dataframe

tidyweather <- weather_select %>%
  pivot_longer(cols = 2:13, names_to = "month", values_to = "delta")
glimpse(tidyweather)

```

## Plotting Information

```{r scatter_plot}
#Preparing the dataset for plotting

tidyweather_new <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

# Plotting the data

 p1 <- ggplot(tidyweather_new, aes(x=date, y = delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_clean() +
  labs (
    title = "Weather Anomalies"
  )

p1
```

```{r facet_wrap}
# Applying a facet wrap to the plot

p2 <- p1 + facet_wrap(~month)

p2

```

```{r intervals}
# Filtering the dataset

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881

# Creating an "interval" variable
  
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

```{r density_plot}
# Creating a density plot

ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.2) +   
  theme_bw() +              
  labs (
    title = "Density Plot for Monthly Temperature Anomalies",
    y     = "Density"     
  )

```
```{r averaging}
# Creating yearly averages

average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
# Finding the mean delta
  
  summarise(annual_average_delta = mean(delta, na.rm=TRUE)) 

average_annual_anomaly
                        
```
```{r averaging1}
# Plotting the data

ggplot(average_annual_anomaly, aes(x=Year, y= annual_average_delta))+
  geom_point()+
  geom_smooth(method = "loess") +
  theme_bw() +
  labs (
    title = "Average Yearly Anomaly",
    y     = "Average Annual Delta"
  )                         

```
## Confidence Interval for `delta`

```{r, calculate_CI_using_formula}
#Finding the confidence interval using a formula

library(infer)
formula_ci <- comparison %>%
  filter(interval == "2011-present") %>%
  summarize(mean_dev=mean(delta, na.rm=TRUE), sd_dev=sd(delta, na.rm=TRUE), count = n(), se_dev=sd_dev/sqrt(count), t_critical = qt(0.975, count-1), margin_of_error = t_critical * se_dev, temp_low = mean_dev - margin_of_error, temp_high = mean_dev + margin_of_error)

formula_ci
```


```{r, calculate_CI_using_bootstrap}

# Finding the confidence interval using bootstrap

boot_temp <- comparison %>%
  filter(interval == "2011-present") %>%
  specify(response = delta) %>%
  generate (reps = 100000, type = "bootstrap") %>%
  calculate (stat = "mean")

  percentile_ci <- boot_temp %>%
    get_confidence_interval(level = 0.975, type = "percentile")
percentile_ci
  
```

## Comment

We have found a confidence interval using two methods. The results show us that since 2011 the average change in global temperature has been 0.97, just under the 1 degree threshold. However, based on the standard error of this dataset we can be 95% certain that the population's mean lies within a range that includes values as high as 1.03 degrees of global temperature change.

# General Social Survey (GSS)

```{r, read_gss_data, cache=TRUE}
gss <- read_csv(here::here("data", "smallgss2016.csv"), 
                na = c("", "Don't know",
                       "No answer", "Not applicable"))

glimpse(gss)
skim(gss)
```

## Instagram and Snapchat, by sex

```{r, insta snapchat, cache=TRUE}

#Creating new variables
social_usage <- gss %>%
  mutate(snap_insta = case_when(snapchat == "Yes" & instagrm == "Yes" ~ "Yes", snapchat == "Yes" | instagrm == "Yes" ~ "Yes", snapchat == "No" & instagrm == "No" ~ "No", snapchat == "NA" & instagrm == "NA" ~ "NA" ))

#Calculating the proportions of Yes's and those who answered
share_yes <- social_usage %>% 
  count(snap_insta == "Yes") %>% 
  mutate(n/sum(n)*100)

share_yes_no <- social_usage %>%
  filter(snap_insta != "NA")

share_yes_no %>% 
  count(snap_insta == "Yes") %>% 
  mutate(n/sum(n)*100)

#Social media usage
social_stats <- social_usage %>%
  group_by(snap_insta) %>%
  count(snap_insta) %>%
  pivot_wider(names_from = snap_insta, values_from = n) %>%
  mutate(proporation_yes = Yes/(Yes+No))

social_stats

#95% confidence interval
ci_share <- social_usage %>%
  group_by(sex, snap_insta) %>%
  count(snap_insta) %>%
  pivot_wider(names_from = snap_insta, values_from = n) %>%
  mutate(mean = Yes/(Yes+No), se = sqrt(mean*(1-mean)/(Yes+No)), t_critical = qt(0.975, (Yes+No)-1), lower = mean - t_critical*se, upper = mean + t_critical*se)

ci_share

#Absolute number of men and women using social media
ci_gender <- social_usage %>%
  group_by(sex, snap_insta) %>%
  count(snap_insta)

ci_gender1 <- social_usage %>%
  group_by(sex, snap_insta) %>%
  count(snap_insta) %>%
  pivot_wider(names_from = snap_insta, values_from = n)

ci_gender1

```

## Twitter, by education level

```{r, twitter, cache=TRUE}

#Turning to Factor variable and non alphabetical
twitter_users <- gss %>%
  group_by(degree) %>%
  count(twitter)

level_order <- c("Lt high school", "High school", "Junior college", "Bachelor", "Graduate")

twitter_count <- gss %>% 
  group_by(degree) %>%
  mutate(degree = factor(degree, levels = level_order))

#Creating new variable bachelor_graduate
twitter_edu <- twitter_count %>%
  mutate(bachelor_graduate = case_when(degree == "Bachelor" | degree == "Graduate" ~ "Yes", degree == "High school" | degree == "Junior college" | degree == "Lt high school" ~ "No", degree == "NA" ~ "NA" ))

#Proportion of bachelor_graduate
twitter_data <- twitter_edu %>%
  group_by(bachelor_graduate, twitter) %>%
  count(bachelor_graduate, twitter) %>%
  filter(bachelor_graduate == "Yes") %>%
  pivot_wider(names_from = twitter, values_from = n) %>%
  mutate(twitter_yes = Yes/(Yes+No))

twitter_data_set <- twitter_edu %>%
  group_by(bachelor_graduate, twitter) %>%
  filter(bachelor_graduate == "Yes", twitter != "NA") %>%
  summarise(count = n()) %>% 
  mutate(per_tw = count/sum(count))

twitter_data_set

#Confidence intervals with 95% of Yes and No, and summary
twitter_CI_yes <- twitter_edu %>%
  group_by(bachelor_graduate, twitter) %>%
  count(bachelor_graduate, twitter) %>%
  pivot_wider(names_from = twitter, values_from = n) %>%
  mutate(mean = Yes/(Yes+No), se = sqrt(mean*(1-mean)/(Yes+No)), t_critical = qt(0.975, (Yes+No)-1), lower = mean - t_critical*se, upper = mean + t_critical*se)

twitter_CI_no <- twitter_edu %>%
  group_by(bachelor_graduate, twitter) %>%
  count(bachelor_graduate, twitter) %>%
  pivot_wider(names_from = twitter, values_from = n) %>%
  mutate(mean = No/(Yes+No), se = sqrt(mean*(1-mean)/(Yes+No)), t_critical = qt(0.975, (Yes+No)-1), lower = mean - t_critical*se, upper = mean + t_critical*se)

twitter_CI_yes

twitter_CI_no

twitter_CI_summary <- twitter_data_set %>%
  summarise(count, per_tw, se = sqrt((per_tw*(1-per_tw)/sum(count))), t_critical = qt(0.975, count-1), margin_of_error = t_critical * se, lower = per_tw - margin_of_error, higher = per_tw + margin_of_error) 

twitter_CI_summary

```

The confidence intervals do not overlap as the lower bound of "No" is higher than the upper bound of "Yes".


## Email usage

```{r, email, cache=TRUE}

#Creating new variable
email_usage <- gss

email_usage[email_usage == "NA"] <- NA

email_visualize <- email_usage %>% 
  na.omit() %>% 
  mutate(emailmin = as.integer(emailmin),
         emailhr = as.integer(emailhr),
         email = emailmin + emailhr*60)

#Visualizing the distribution of new variable
ggplot(data = email_visualize, aes(x = email)) + geom_boxplot() + labs(title = "Distribution on time spent on email weekly", x = "Minutes per week", y = "") + theme_clean() + NULL

ggplot(data = email_visualize, aes(x = email)) + geom_histogram() + labs(title = "Distribution on time spent on email weekly", x = "Minutes per week", y = "Number of people") + theme_clean() + NULL
                        
#95% CI bootstrap for mean time Americans spend on email weekly
library(infer)

email_95 <- email_visualize %>%
  specify(response = email) %>%
  generate (reps = 1000, type = "bootstrap") %>%
  calculate (stat = "mean")

email_95_ci <- email_95 %>%
    get_confidence_interval(level = 0.95, type = "percentile")

email_95_ci %>%
  mutate(lower_ci = lower_ci/60, upper_ci = upper_ci/60)

#99% CI bootstrap for mean time Americans spend on email weekly
email_99 <- email_visualize %>%
  specify(response = email) %>%
  generate (reps = 1000, type = "bootstrap") %>%
  calculate (stat = "mean")

email_99_ci <- email_99 %>%
    get_confidence_interval(level = 0.99, type = "percentile")

email_99_ci
```

Considering several outliers it is better to use the median number as a measure of the typical amount American spend on their email weekly. The mean would be skewed to the right.

A 99% interval would be wider than the 95% one because it includes more variables of the sample. 

# Trump's Approval Margins

```{r, cache=TRUE}
# Import approval polls data
approval_polllist <- read_csv(here::here('data', 'approval_polllist.csv'))

glimpse(approval_polllist)

# Use `lubridate` to fix dates, as they are given as characters.

library(lubridate)

approval_polllist$modeldate<-mdy(approval_polllist$modeldate)
approval_polllist$startdate<-mdy(approval_polllist$startdate)
approval_polllist$enddate<-mdy(approval_polllist$enddate)
approval_polllist$createddate<-mdy(approval_polllist$createddate)

```

## Create a plot

```{r, cache=TRUE}
#Adding net approval
president_approval <- approval_polllist %>%
  mutate(net_approval = adjusted_approve - adjusted_disapprove)

#Calculating statistics
net_approval_weekly <- president_approval %>%
  filter(subgroup == "Voters") %>%
  group_by(week_number = isoweek(enddate), year=year(enddate)) %>% 
  summarise(mean_app = mean(net_approval, na.rm = TRUE), sd_app = sd(net_approval, na.rm = TRUE), count_app = n(), se_app = sd_app / sqrt(count_app), t_critical = qt(0.975, count_app-1) , margin_of_error = t_critical * se_app, app_low = mean_app - margin_of_error, app_high = mean_app + margin_of_error) %>%
  filter(count_app>1)

#Plotting
ggplot(data = net_approval_weekly, aes(x = week_number,fill=as.factor(year))) + scale_fill_manual(values = c("red","#a1d99b","#66FFFF","#d4b9da")) + geom_line(aes(y=app_low))+geom_line(aes(y=mean_app)) + 
  geom_line(aes(y=app_high))+geom_point(aes(y=mean_app)) + facet_wrap(~year) + geom_ribbon(aes(ymin=app_low,ymax=app_high),alpha=0.3)  + geom_hline(yintercept=0, size=1.0, color="orange") + scale_y_continuous(breaks=c(-20,-17.5,-15,-12.5,-10,-7.5,-5,-2.5,0,2.5,5.0,7.5)) + scale_x_continuous(breaks=c(0, 13,26,39,52)) + labs(title="Estimating Net Approval (approve-disapprove) for Donald Trump
",subtitle="Weekly average of all polls",x="Week of the year",y="Average net approval (%)") + theme(text = element_text(size = 11),legend.position = "none", panel.background = element_rect(fill = "white", colour = "white", size = 0.5, linetype = "solid"), panel.grid.major = element_line(size = 0.5, linetype = 'solid', colour = "light grey"), panel.grid.minor = element_line(size = 0.5, linetype = 'solid', colour = " light grey"))
    

```

```{r trump_margins, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "trump_approval_margin.png"), error = FALSE)
```

## Compare Confidence Intervals

```{r, cache=TRUE}

#Filtering for week 15 year 2020 in the table
net_approval_week15 <- net_approval_weekly %>%
  filter(week_number == 15, year == 2020)

net_approval_week15

#Filtering for week 34 year 2020 in the table
net_approval_week34 <- net_approval_weekly %>%
  filter(week_number == 34, year == 2020)

net_approval_week34

```
Looking at the graph, the confidence interval of the later week (week 34) is larger. There might be two reasons for this development, standard deviation and sample size. Variability might be increased and/ or the sample size might be reduced. Checking the table, we are able to identify that standard deviation indeed increased and sample size indeed decreased. 

# Gapminder revisited

```{r, get_data, cache=TRUE}

# load gapminder HIV data
hiv <- read_csv(here::here("data","adults_with_hiv_percent_age_15_49.csv"))
life_expectancy <- read_csv(here::here("data","life_expectancy_years.csv"))

# get World bank data using wbstats
indicators <- c("SP.DYN.TFRT.IN","SE.PRM.NENR", "SH.DYN.MORT", "NY.GDP.PCAP.KD")

library(wbstats)

worldbank_data <- read_csv(here::here("data","worldbank_data.csv"))

# get a dataframe of information regarding countries, indicators, sources, regions, indicator topics, lending types, income levels,  from the World Bank API 
countries <-  read.csv(here::here("data","countries.csv"))

```

Working with the following three data frames:

* hiv
* life_expectancy
* worldbank_data

As a first step, we need to tidy the data of hiv and life_expectancy.
```{r, cache=TRUE}
#glimpse(hiv)
#glimpse(life_expectancy)
#glimpse(worldbank_data)
```
```{r, cache=TRUE}
library(countrycode)

#tidy the data fo hiv and life_expectancy
hiv_tidy <- hiv %>%
  pivot_longer(cols = c("1979":"2011"),
               names_to = "date" , 
               values_to = "hiv_prevalence", 
               values_drop_na= FALSE)


life_expectancy_tidy <- life_expectancy %>%
  pivot_longer(cols = c("1800":"2100"),
   names_to = "date" , 
   values_to = "life_expectancy", 
   values_drop_na= FALSE)

#Preparation for left join 
life_expectancy_tidy$date <- as.double(life_expectancy_tidy$date)
hiv_tidy$date <- as.double(hiv_tidy$date)

#Left join the data frames
three_combined <- left_join(worldbank_data,life_expectancy_tidy) %>%
  left_join(.,hiv_tidy) %>%
   mutate(region= countrycode(sourcevar = country, origin = "country.name",destination = "region"))

three_combined


```

## Relationship between HIV prevalence and life expectancy

```{r, cache=TRUE}

#Plotting two graphs, one for all countries, one focusing on the different regions
ggplot(data = three_combined, aes(x = hiv_prevalence, y= life_expectancy )) + geom_point(alpha=0.4) + geom_smooth() + labs(title="Negative correlation between HIV prevalence and life expectancy",subtitle="HIV prevalence and life expectancy",x="HIV prevalence",y="Life expectancy") + theme_classic()

ggplot(data = three_combined, aes(x = hiv_prevalence, y= life_expectancy )) + geom_point(alpha=0.4) + geom_smooth(method="gam") + labs(title="Negative correlation between HIV prevalence and life expectancy",subtitle="HIV prevalence and life expectancy",x="HIV prevalence",y="Life expectancy") + theme_classic() + facet_wrap(~region, scales="free")

#cleaning data before calculating correlation coefficient
correlation_hiv_lifeexpectancy <- three_combined %>%
  filter (!is.na(hiv_prevalence & life_expectancy)) %>%
  select(hiv_prevalence, life_expectancy) %>% 
  cor()

correlation_hiv_lifeexpectancy
```

Overall, HIV and life expectancy are negatively correlated (p=-0.531). For regions like Sub-Saharan Africa the relationship seems rather clear, for other regions like East Asia & Pacific it's less obvious. Thus, in order to confirm a direct causal effect, a deeper investigation is necessary and also other variables should be considered.

## Relationship between fertility rate and GDP per capita

```{r, cache=TRUE}
#Plotting two graphs, one for all countries, one focusing on the different regions
ggplot(data = three_combined, aes(x = NY.GDP.PCAP.KD, y= SP.DYN.TFRT.IN)) + geom_point(alpha=0.4) + geom_smooth() + labs(title="Better living standard and healthcare access leads to less children",subtitle="GDP per capita and fertility rate",x="GDP per capita",y="Fertility rate") + theme_classic()

ggplot(data = three_combined, aes(x = NY.GDP.PCAP.KD, y= SP.DYN.TFRT.IN)) + geom_point(alpha=0.4) + geom_smooth(method = lm) + labs(title="Better living standard and healthcare access leads to less children",subtitle="GDP per capita and fertility rate",x="GDP per capita",y="Fertility rate") + theme_classic() + facet_wrap(~region, scales="free")

#cleaning data before calculating correlation coefficient
correlation_fertility_gdp <- three_combined %>%
  filter (!is.na(NY.GDP.PCAP.KD & SP.DYN.TFRT.IN)) %>%
  select(NY.GDP.PCAP.KD, SP.DYN.TFRT.IN) %>% 
  cor()

correlation_fertility_gdp


```

Fertility rate and GDP per capita seem to be negatively correlated. Especially Sub-Saharan Africa and South Asia and Latin America & Caribbean are good examples for this relationship. Higher income might lead to better access to healthcare and contraceptives. This anaylsis is underlined by a correlation coefficient of -0.509

## Regions having the most observations with missing HIV data 

```{r, cache=TRUE}

#Filtering for 1979-2011 because hiv data is only available for this time frame and then calculating both absolute and relative value of missing hiv data
hiv_miss <- three_combined %>%
  group_by(region) %>%
  filter(date>=1979) %>%
  filter(date<=2011)  %>%
   summarize(NA_hiv = sum(is.na(hiv_prevalence)), NA_percentage = round (NA_hiv/n(), digits = 4)) %>%
  arrange(desc(NA_hiv))

#Plot - absolute values
ggplot(data = hiv_miss, aes(x = reorder(region, NA_hiv), y = NA_hiv)) + geom_col() + coord_flip() + geom_text(aes(label=NA_hiv), vjust=0.5,hjust=2,angle=0, color="red", size=3) + labs(title="Missing HIV prevalence observations",subtitle="Per region / absolute values", y="Missing HIV data",x="") + theme_economist_white()

#Plot - relative values
ggplot(data = hiv_miss, aes(x = reorder(region, NA_percentage), y = NA_percentage)) + geom_col() + coord_flip() + geom_text(aes(label=scales::percent(NA_percentage)), vjust=0.5,hjust=2,angle=0, color="red", size=3) + labs(title="Missing HIV prevalence observations",subtitle="Per region / relative values", y="Missing HIV data",x="") + theme_economist_white()
```

Considering absolute values, Europe & Central Asia are leading with the most missing values overall. However, considering proportion, it's East Asia & Pacific which are leading. Using the second analysis might be more helpful because not all regions do have the same number of countries.

## Top 5 countries with the greatest/lowest decrease in mortality rate per region

```{r, cache=TRUE}

#Preparing data by filtering and untidying
improvement <- three_combined %>%
                select(c("country","region","date","SH.DYN.MORT")) %>% 
                filter(date== "1960"| date=="2016") %>%
                pivot_wider(names_from="date", values_from="SH.DYN.MORT")

colnames(improvement) = c("country","region","start","end")

#Calculating change over the years
impro_mortality <- improvement %>% 
mutate(change_over_years=(end-start)/start) %>% 
group_by(region) %>%
summarize(country,change_over_years) %>%
arrange(region,desc(change_over_years))

#top 5 per region
best_five <- impro_mortality %>%
            slice_min(order_by= change_over_years,n=5) %>%
            summarize(country,change_over_years)

#Plotting
ggplot(best_five,aes(x=reorder(country,change_over_years),y=abs(change_over_years))) +
  geom_col(fill="dark green") + coord_flip()+ facet_wrap(~region,scales="free") + 
  labs(title="Countries with the greatest decrease in mortality 1960-2016",subtitle="Top 5 countries per region ",y="Change in mortality rate between 1960 and 2016",x="") +
  geom_text(aes(label=scales::percent(change_over_years)), vjust=1,hjust=2, angle=0, color="black", size=2.5) 

#bottom 5 per region  
worst_five <- impro_mortality %>%
    slice_max(order_by= change_over_years,n=5) %>%
  summarize(country,change_over_years)

#Plotting
ggplot(worst_five,aes(x=reorder(country,change_over_years),y=abs(change_over_years))) +
  geom_col(fill="dark red") + coord_flip()+ facet_wrap(~region,scales="free") + 
  labs(title="Countries with the worst decrease in mortality 1960-2016",subtitle="Bottom 5 countries per region ",y="Change in mortality rate between 1960 and 2016",x="") +
  geom_text(aes(label=scales::percent(change_over_years)), vjust=1,hjust=2, angle=0, color="black", size=2.5) 
```

It is important to mention that the study has various limits. First, many countries do not have data for the specific time frame leading to a potential fabrication of the data. A solution might have been to use the first and last available data points of each country, however, this would have greated another issue considering the fact of comparing different times frames in that case. Further, it is potentially not enough to compare only relative values. It is not a surprise that the most developed countries had the lowest margins of improvement considering their already high level at the beginning. 

## Relationship between primary school enrollment and fertility rate

```{r, cache=TRUE}

#Plotting
ggplot(data = three_combined, aes(x = SE.PRM.NENR, y= SP.DYN.TFRT.IN)) + geom_point(alpha=0.4) + geom_smooth(method = lm) + labs(title="Better living standard and healthcare access leads to less children",subtitle="Primary school enrollment and fertility rate",x="Primary school enrollment",y="Fertility rate") + theme_classic() + facet_wrap(~region, scales="free")

#cleaning data before calculating correlation coefficient
correlation_primary_fertility <- three_combined %>%
  filter (!is.na(SE.PRM.NENR & SP.DYN.TFRT.IN)) %>%
  select(SE.PRM.NENR, SP.DYN.TFRT.IN) %>% 
  cor()

correlation_primary_fertility

```

Considering the correlation coefficient of -0.716, there seems to be a clear neagtive correlation between these two variables. This reflects the general opinion that besides a higher GDP especially education has a very strong impact on the fertility rate. Usually better education leads to less children in a society. This is perfectly visualized in the Sub-Saharan Africa graph or the South Asia graph.

# Challenge 1: CDC COVID-19 Public Use Data


```{r, cache=TRUE}
# file contains 11 variables and 3.66m rows and is well over 380Mb. 
# It will take time to download

# URL link to CDC to download data
url <- "https://data.cdc.gov/api/views/vbim-akqf/rows.csv?accessType=DOWNLOAD"

covid_data <- vroom::vroom(url)%>% # If vroom::vroom(url) doesn't work, use read_csv(url)
  clean_names()


```


## Covid death % by age group, sex and presence of co-morbidities

```{r, cache=TRUE}

#glimpse(covid_data)
#Data cleaning

cleaning_covid_data <- covid_data %>% 
  filter(medcond_yn != "Missing") %>% 
  filter(medcond_yn != "Unknown") %>% 
  filter(death_yn != "Unknown") %>% 
  filter(death_yn != "Missing") %>%
  filter(age_group != "Unknown") %>% 
  filter(sex != "Missing") %>% 
  filter(sex != "Unknown") %>% 
  filter(sex != "Other")

#glimpse(cleaning_covid_data)

#Adding the death percentage

covid_deaths <- cleaning_covid_data %>% 
  group_by(age_group,sex,medcond_yn,death_yn) %>% 
  count() %>% 
  pivot_wider(names_from = death_yn, values_from = n) %>%
  mutate(death_percentage = Yes/(Yes+No))

#glimpse(covid_deaths)

```

```{r, cache=TRUE}
#Change label names
covid_deaths$medcond_yn_names <- factor(covid_deaths$medcond_yn,
levels = c("Yes", "No"),
labels = c("With comorbidities", "Without comorbidities"))

#glimpse(covid_deaths)

#Graphical reflection
ggplot(data = covid_deaths, aes(x = age_group, y = death_percentage)) + geom_col(fill = "#6b7ca4", show.legend = FALSE) +  geom_text(aes(label = round(death_percentage*100, 1), hjust = 0.6)) + labs(title = "Covid death % by age group, sex and presence of co-morbidities", caption = "Source:CDC") + theme_bw() + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) + scale_y_continuous(labels = scales::percent) + coord_flip() + facet_grid(medcond_yn_names ~ sex)


```

## Covid death rate by age group, sex and whether patient was admitted to ICU

```{r, cache=TRUE}

#glimpse(covid_data)
#Data cleaning

cleaning_covid_icu <- covid_data %>% 
  filter(icu_yn != "Missing") %>% 
  filter(icu_yn != "Unknown") %>%
  filter(age_group != "Unknown") %>% 
  filter(sex != "Missing") %>% 
  filter(sex != "Unknown") %>% 
  filter(sex != "Other") %>% 
  filter(death_yn != "Unknown") %>% 
  filter(death_yn != "Missing")

#glimpse(cleaning_covid_icu)

#Adding the death percentage

icu_covid <- cleaning_covid_icu %>% 
  group_by(age_group,sex,icu_yn,death_yn) %>% 
  count() %>% 
  pivot_wider(names_from = death_yn, values_from = n) %>% 
  summarize(death_percentage = Yes/(Yes+No))

#glimpse(icu_covid)
```

```{r, cache=TRUE}
#Change label names
icu_covid$icu_yn_names <- factor(icu_covid$icu_yn,
levels = c("Yes", "No"),
labels = c("Admitted to ICU", "No ICU"))

#glimpse(icu_covid)

#Graphical reflection
ggplot(data = icu_covid, aes(x = age_group, y = death_percentage)) + geom_col(fill = "#fe9481", show.legend = FALSE) + geom_text(aes(label = round(death_percentage*100, 1), hjust = 0.6)) + labs(title = "Covid death % by age group, sex and whether patient was admitted to ICU", caption = "Source:CDC") + theme_bw() + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) + scale_y_continuous(labels = scales::percent) + coord_flip() + facet_grid(icu_yn_names ~ sex)
```

```{r covid_challenge, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "covid_death_rate_comorbidities.png"), error = FALSE)
knitr::include_graphics(here::here("images", "covid_death_rate_icu.png"), error = FALSE)
```


# Challenge 2: Excess rentals in TfL bike sharing

```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```

```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_monthly.png"), error = FALSE)
```

```{r tfl_percent_change, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "tfl_weekly.png"), error = FALSE)
```
For both of these graphs, we will calculate the expected number of rentals per week or month between 2015-2019 and then, see how each week/month of 2020 compares to the expected rentals.

We need to use the mean number of bike rentals to calculated expected rentals as the mean considers all values and not only middle ones and we do not have many outliers in the sample.

## Monthly changes in bike rentals
```{r first_figure, fig.width = 10, fig.height=7}

#calculate average bike hires in 2015-2019 by month (expected rentals)
expected_rentals <- bike %>%
  filter(year >=2015 & year <= 2019) %>%
  group_by(month) %>%
  summarize(avg_hires = sum(bikes_hired)/n())

#calculate average bike hires by month including 2020 data (actual rentals)
actual_rentals <- bike %>%
  filter(year >= 2015) %>%
  group_by(year, month) %>%
  summarize(actual_hires = sum(bikes_hired)/n()) %>%
  inner_join(expected_rentals, by = "month")

#plot the graph of change in monthly expected rentals vs actual rentals in 2015-2020
ggplot(actual_rentals, aes(x = month, y = avg_hires)) +
  geom_ribbon(aes(ymin = actual_hires, ymax = pmin(actual_hires, avg_hires), group = year,
                  alpha = 0.5),
              show.legend = FALSE,
              fill = '#4CC076', color = "black", size=0.15) +
  geom_ribbon(aes(ymin = avg_hires, ymax = pmin(actual_hires, avg_hires), group = year,
                  alpha = 0.5),
              show.legend = FALSE,
              fill = "#CD8383", color = "black", size=0.15) +
  geom_line(size = 0.5, color = "blue", aes(y = avg_hires, group = 1)) +
  labs(title = "Monthly changes in TfL bike rentals",
       subtitle = "Change from monthly average shown in blue \n and calculated between 2015-2019",
       y = "Bike rentals",x=NULL) +
  facet_wrap(~year) +
  theme(strip.background = element_rect(fill="white", size = 0.5),
              strip.text = element_text(size=7),
              panel.background = element_rect(fill = "white"),
              panel.grid = element_line(colour = "#f0f0f0"),
              plot.title = element_text(size = 7),
              axis.title.y = element_blank()) + theme(plot.title = element_text(size = 14)) 

```

## Weekly changes in bike rentals
```{r weekly_changes, fig.width=10, fig.height =8}

library(FreqProf)

#calculate average bike hires in 2015-2019 by week (expected rentals)
expected_rentals <- bike %>%
  filter(year >=2015 & year <= 2019) %>%
  group_by(week) %>%
  summarize(avg_hires = sum(bikes_hired)/n())

#calculate average bike hires by week including 2020 data (actual rentals) and % change between expected and actual rentals
actual_rentals <- bike %>%
  filter(year >= 2015) %>%
  group_by(year, week) %>%
  summarize(actual_hires = sum(bikes_hired)/n()) %>%
  inner_join(expected_rentals, by = "week") %>%
  summarize(week = week, per_change = (actual_hires-avg_hires)/avg_hires)


#plot the graph of % change in weekly expected rentals vs actual rentals in 2015-2020
ggplot(actual_rentals, aes(x = week,y = per_change)) +
   annotate("rect", fill = "#E0E0E0", alpha = 0.5, 
        xmin = 13, xmax = 26,
        ymin = -Inf, ymax = Inf)+  
  annotate("rect", fill = "#E0E0E0", alpha = 0.5, 
        xmin = 39, xmax = 53,
        ymin = -Inf, ymax = Inf)+
   geom_ribbon(aes(ymin = pmin(0, per_change), ymax = 0, group = year,
                  alpha = 0.5),
              show.legend = FALSE,
              fill = "#CD8383", color = "grey") +
  geom_ribbon(aes(ymin = 0, ymax = pmax(0, per_change), group = year,
                  alpha = 0.5),
              show.legend = FALSE,
              fill = '#4CC076', color = "grey") +
  geom_line(size = 0.2, color = "black", aes(y = per_change, group = 1)) +
  labs(title = "Weekly changes in TfL bike rentals",
       subtitle = "% Changes from weekly averages \n calculated between 2015-2019",
       y = "Bike rentals") +
    scale_y_continuous(labels = scales::percent) +
     scale_x_continuous(breaks=c(13,26, 39, 53))+
  facet_wrap(~year) +
  theme(strip.background = element_rect(fill="white", size = 0.5),
              strip.text = element_text(size=8),
              panel.background = element_rect(fill = "white"),
              panel.grid = element_line(colour = "#f0f0f0"),
              plot.title = element_text(size = 8),
              axis.title.y = element_blank()) + theme(plot.title = element_text(size = 14))+
  geom_rug(aes(colour=ifelse(per_change>=0,">=0","<0")),sides="b",alpha=0.5)+
    scale_colour_manual(values=c("#CD8383","#4CC076"), guide=FALSE)
  
```


# Details

- Who did you collaborate with: Aman Sharma, Christoph Sieker, Kasia Gasiewska, Peter Moravec, Philippe Schrage, Satyam Gorry
- Approximately how much time did you spend on this problem set: 60 hours
- What, if anything, gave you the most trouble: Challenge 2

# Rubric

Check minus (1/5): Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. 

Check (3/5): Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). 

**Check plus (5/5): Finished all components of the assignment correctly and addressed both challenges. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output.**